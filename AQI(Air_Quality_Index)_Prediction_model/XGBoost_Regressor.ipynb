{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we are using XGBoost Regressor with and without Hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>TM</th>\n",
       "      <th>Tm</th>\n",
       "      <th>SLP</th>\n",
       "      <th>H</th>\n",
       "      <th>VV</th>\n",
       "      <th>V</th>\n",
       "      <th>VM</th>\n",
       "      <th>PM 2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.4</td>\n",
       "      <td>30.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>284.795833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.4</td>\n",
       "      <td>30.3</td>\n",
       "      <td>16.9</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>219.720833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>16.9</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>182.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>17.6</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>7.6</td>\n",
       "      <td>154.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.1</td>\n",
       "      <td>30.6</td>\n",
       "      <td>17.4</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>223.208333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T    TM    Tm   SLP    H   VV    V   VM      PM 2.5\n",
       "0  23.4  30.3  19.0  59.0  0.0  6.3  4.3  5.4  284.795833\n",
       "1  22.4  30.3  16.9  57.0  0.0  6.9  3.3  7.6  219.720833\n",
       "2  24.0  31.8  16.9  51.0  0.0  6.9  2.8  5.4  182.187500\n",
       "3  22.8  30.2  17.6  55.0  0.0  7.7  3.7  7.6  154.037500\n",
       "4  23.1  30.6  17.4  46.0  0.0  6.9  3.3  5.4  223.208333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/Real-Data/Real_Combine.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "Y=data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.2.1-py3-none-macosx_10_13_x86_64.macosx_10_14_x86_64.macosx_10_15_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/anaconda3/envs/p36_shv_new/lib/python3.6/site-packages (from xgboost) (1.5.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/p36_shv_new/lib/python3.6/site-packages (from xgboost) (1.17.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.2.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = xgb.XGBRegressor()\n",
    "regressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on Train Dataset 0.998664792887286\n"
     ]
    }
   ],
   "source": [
    "print('Score on Train Dataset',regressor.score(X_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on Test Dataset 0.2862652158761383\n"
     ]
    }
   ],
   "source": [
    "print('Score on Test Dataset',regressor.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above demonstrates that this is a perfect Overfit problem!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do an Hyper parameter tuning to see, if we can prune the DT incolved in XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "             validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(100,1200,12)]\n",
    "\n",
    "#Randomized Search CV\n",
    "learning_rate = ['0.05','0.1','0.2','0.3','0.5','0.6']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(5,30,6)]\n",
    "\n",
    "subsample =[0.7,0.6,0.8]\n",
    "\n",
    "min_child_weight = [3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate':learning_rate,\n",
    "               'max_depth':max_depth,\n",
    "               'subsample':subsample,\n",
    "               'min_child_weight':min_child_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 1.46 s, total: 1min 28s\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None, gamma=None,\n",
       "                                          gpu_id=None, importance_type='gain',\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=None,\n",
       "                                          n_estimators=100, n_jobs...\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   param_distributions={'learning_rate': ['0.05', '0.1', '0.2',\n",
       "                                                          '0.3', '0.5', '0.6'],\n",
       "                                        'max_depth': [5, 10, 15, 20, 25, 30],\n",
       "                                        'min_child_weight': [3, 4, 5, 6, 7],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000, 1100,\n",
       "                                                         1200],\n",
       "                                        'subsample': [0.7, 0.6, 0.8]},\n",
       "                   scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "regressor = xgb.XGBRegressor()\n",
    "\n",
    "# Use the XGBRegressor and find the best fit\n",
    "xg_random = RandomizedSearchCV(regressor,param_distributions=random_grid,scoring='neg_mean_squared_error')\n",
    "\n",
    "xg_random.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.8,\n",
       " 'n_estimators': 100,\n",
       " 'min_child_weight': 5,\n",
       " 'max_depth': 20,\n",
       " 'learning_rate': '0.1'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4640.528380990496"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-59.487871490170285"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_random.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_hyp_tuned = xgb.XGBRegressor(subsample=0.8,n_estimators=100,min_child_weight=5,max_depth=20,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_hyp_tuned.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916080573585102"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_hyp_tuned.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9778611833090686"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_hyp_tuned.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As seen above now, we could see that the XGBoostRegressor have performed the best after Hyper Prameter tuning. My other observation was the computation time taken in XGBoost was much less compared to that of DT and RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoute Error =  6.3511089261823805\n",
      "Mean squared Error =  154.75014258525428\n",
      "R^2 =  0.9778611833090686\n",
      "RMSE =  12.439861035608649\n"
     ]
    }
   ],
   "source": [
    "#Model evaluation for Decision Tree Regressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred_test = regressor_hyp_tuned.predict(X_test)\n",
    "\n",
    "print(\"Mean Absoute Error = \",mean_absolute_error(Y_test,y_pred_test))\n",
    "\n",
    "print(\"Mean squared Error = \",mean_squared_error(Y_test,y_pred_test))\n",
    "\n",
    "print(\"R^2 = \",r2_score(Y_test,y_pred_test))\n",
    "\n",
    "print(\"RMSE = \",np.sqrt(mean_squared_error(Y_test,y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = 'XGBoostRegressor.sav'\n",
    "\n",
    "pickle.dump(regressor_hyp_tuned,open(file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
